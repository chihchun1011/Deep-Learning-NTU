{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"17YiaMOzi-qtXEJNnyfp1OQiGYVAN7MQV","timestamp":1616918000216},{"file_id":"https://github.com/ga642381/ML2021-Spring/blob/main/HW02/HW02-1.ipynb","timestamp":1615728360887}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"OYlaRwNu7ojq"},"source":["# **Homework 2-1 Phoneme Classification**"]},{"cell_type":"markdown","metadata":{"id":"emUd7uS7crTz"},"source":["## The DARPA TIMIT Acoustic-Phonetic Continuous Speech Corpus (TIMIT)\n","The TIMIT corpus of reading speech has been designed to provide speech data for the acquisition of acoustic-phonetic knowledge and for the development and evaluation of automatic speech recognition systems.\n","\n","This homework is a multiclass classification task,\n","we are going to train a deep neural network classifier to predict the phonemes for each frame from the speech corpus TIMIT.\n","\n","link: https://academictorrents.com/details/34e2b78745138186976cbc27939b1b34d18bd5b3"]},{"cell_type":"markdown","metadata":{"id":"KVUGfWTo7_Oj"},"source":["## Download Data\n","Download data from google drive, then unzip it.\n","\n","You should have `timit_11/train_11.npy`, `timit_11/train_label_11.npy`, and `timit_11/test_11.npy` after running this block.<br><br>\n","`timit_11/`\n","- `train_11.npy`: training data<br>\n","- `train_label_11.npy`: training label<br>\n","- `test_11.npy`:  testing data<br><br>\n","\n","**notes: if the google drive link is dead, you can download the data directly from Kaggle and upload it to the workspace**\n","\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OzkiMEcC3Foq","executionInfo":{"status":"ok","timestamp":1616999216991,"user_tz":-480,"elapsed":27897,"user":{"displayName":"Sara Kuo","photoUrl":"","userId":"17378102041433468406"}},"outputId":"d4992e61-82e0-4449-c11e-925f05a52749"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","# !gdown --id '1HPkcmQmFGu-3OknddKIa5dNDsR05lIQR' --output data.zip\n","# !cp ./data.zip /content/drive/MyDrive/data.zip\n","\n","!cp /content/drive/MyDrive/data.zip ./\n","!unzip data.zip\n","!ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Archive:  data.zip\n","replace timit_11/train_11.npy? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n","replace timit_11/test_11.npy? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n","replace timit_11/train_label_11.npy? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n","data.zip  drive  model0.ckpt  model1.ckpt  sample_data\ttimit_11\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_L_4anls8Drv"},"source":["## Preparing Data\n","Load the training and testing data from the `.npy` file (NumPy array)."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IJjLT8em-y9G","executionInfo":{"status":"ok","timestamp":1616999376960,"user_tz":-480,"elapsed":178702,"user":{"displayName":"Sara Kuo","photoUrl":"","userId":"17378102041433468406"}},"outputId":"84a329a1-f842-4fa2-cd41-1c550fb32c98"},"source":["import numpy as np\n","\n","print('Loading data ...')\n","\n","data_root='./timit_11/'\n","train = np.load(data_root + 'train_11.npy')\n","train_label = np.load(data_root + 'train_label_11.npy')\n","test = np.load(data_root + 'test_11.npy')\n","\n","print('Size of training data: {}'.format(train.shape))\n","print('Size of testing data: {}'.format(test.shape))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading data ...\n","Size of training data: (1229932, 429)\n","Size of testing data: (451552, 429)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"us5XW_x6udZQ"},"source":["## Create Dataset"]},{"cell_type":"code","metadata":{"id":"Fjf5EcmJtf4e"},"source":["import torch\n","from torch.utils.data import Dataset\n","\n","class TIMITDataset(Dataset):\n","    def __init__(self, X, y=None): #x:feature y:label\n","        self.data = torch.from_numpy(X).float()\n","        if y is not None:\n","            y = y.astype(np.int)\n","            self.label = torch.LongTensor(y)\n","        else:\n","            self.label = None\n","\n","    def __getitem__(self, idx):\n","        if self.label is not None:\n","            return self.data[idx], self.label[idx]\n","        else:\n","            return self.data[idx]\n","\n","    def __len__(self):\n","        return len(self.data)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"otIC6WhGeh9v"},"source":["Split the labeled data into a training set and a validation set, you can modify the variable `VAL_RATIO` to change the ratio of validation data."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sYqi_lAuvC59","executionInfo":{"status":"ok","timestamp":1616999378073,"user_tz":-480,"elapsed":1075,"user":{"displayName":"Sara Kuo","photoUrl":"","userId":"17378102041433468406"}},"outputId":"a4720dc5-6c8f-466a-d5ca-6efda821d7dc"},"source":["VAL_RATIO = 0 #0 train all data\n","\n","percent = int(train.shape[0] * (1 - VAL_RATIO))\n","train_x, train_y, val_x, val_y = train[:percent], train_label[:percent], train[percent:], train_label[percent:]\n","print('Size of training set: {}'.format(train_x.shape))\n","print('Size of validation set: {}'.format(val_x.shape))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Size of training set: (1229932, 429)\n","Size of validation set: (0, 429)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nbCfclUIgMTX"},"source":["Create a data loader from the dataset, feel free to tweak the variable `BATCH_SIZE` here."]},{"cell_type":"code","metadata":{"id":"RUCbQvqJurYc"},"source":["BATCH_SIZE = 128\n","\n","from torch.utils.data import DataLoader\n","\n","train_set = TIMITDataset(train_x, train_y)\n","val_set = TIMITDataset(val_x, val_y)\n","train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True) #only shuffle the training data\n","val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_SY7X0lUgb50"},"source":["Cleanup the unneeded variables to save memory.<br>\n","\n","**notes: if you need to use these variables later, then you may remove this block or clean up unneeded variables later<br>the data size is quite huge, so be aware of memory usage in colab**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y8rzkGraeYeN","executionInfo":{"status":"ok","timestamp":1616999380115,"user_tz":-480,"elapsed":3106,"user":{"displayName":"Sara Kuo","photoUrl":"","userId":"17378102041433468406"}},"outputId":"4ec90971-813d-41b6-d48a-210528b5d23b"},"source":["import gc\n","\n","del train, train_label, train_x, train_y, val_x, val_y\n","gc.collect()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["167"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"IRqKNvNZwe3V"},"source":["## Create Model"]},{"cell_type":"markdown","metadata":{"id":"FYr1ng5fh9pA"},"source":["Define model architecture, you are encouraged to change and experiment with the model architecture."]},{"cell_type":"code","metadata":{"id":"lbZrwT6Ny0XL"},"source":["import torch\n","import torch.nn as nn\n","\n","class Classifier1(nn.Module):\n","    def __init__(self):\n","        super(Classifier1, self).__init__()\n","\n","        self.net = nn.Sequential(\n","            nn.Linear(429, 2048),\n","            nn.ReLU(),\n","            nn.Dropout(0.4),\n","\n","            nn.Linear(2048,1024),\n","            nn.ReLU(),\n","            nn.Dropout(0.4),\n","\n","            nn.Linear(1024, 728),\n","            nn.ReLU(),\n","            nn.Dropout(0.4),\n","\n","            nn.Linear(728, 512),\n","            nn.ReLU(),\n","            nn.Dropout(0.4),\n","\n","            nn.Linear(512, 256),\n","            nn.ReLU(),\n","            nn.Dropout(0.4),\n","\n","            nn.Linear(256, 128),\n","            nn.ReLU(),\n","            nn.Dropout(0.4),\n","\n","\n","            nn.Linear(128, 39)\n","\n","        )\n","\n","    def forward(self, x):\n","\n","      return self.net(x).squeeze(1)\n","\n","class Classifier2(nn.Module):\n","    def __init__(self):\n","        super(Classifier2, self).__init__()\n","\n","        self.net = nn.Sequential(\n","            nn.Linear(429, 2000),\n","            nn.ReLU(),\n","            nn.Dropout(0.4),\n","\n","            nn.Linear(2000,2000),\n","            nn.ReLU(),\n","            nn.Dropout(0.4),\n","\n","            nn.Linear(2000, 80),\n","            nn.ReLU(),\n","            nn.Dropout(0.4),\n","\n","            nn.Linear(80, 2000),\n","            nn.ReLU(),\n","            nn.Dropout(0.4),\n","\n","            nn.Linear(2000, 39)\n","        )\n","\n","    def forward(self, x):\n","\n","      return self.net(x).squeeze(1)\n","\n","class Classifier3(nn.Module):\n","    def __init__(self):\n","        super(Classifier3, self).__init__()\n","\n","        self.net = nn.Sequential(\n","            nn.Linear(429, 1024),\n","            nn.ReLU(),\n","            nn.Dropout(0.4),\n","\n","            nn.Linear(1024,2048),\n","            nn.ReLU(),\n","            nn.Dropout(0.4),\n","\n","            nn.Linear(2048, 728),\n","            nn.ReLU(),\n","            nn.Dropout(0.4),\n","\n","            nn.Linear(728, 512),\n","            nn.ReLU(),\n","            nn.Dropout(0.4),\n","\n","            nn.Linear(512, 128),\n","            nn.ReLU(),\n","            nn.Dropout(0.4),\n","\n","            nn.Linear(128, 39)\n","\n","        )\n","\n","    def forward(self, x):\n","\n","      return self.net(x).squeeze(1)\n",""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VRYciXZvPbYh"},"source":["## Training"]},{"cell_type":"code","metadata":{"id":"y114Vmm3Ja6o"},"source":["#check device\n","def get_device():\n","  return 'cuda' if torch.cuda.is_available() else 'cpu'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sEX-yjHjhGuH"},"source":["Fix random seeds for reproducibility."]},{"cell_type":"code","metadata":{"id":"88xPiUnm0tAd"},"source":["# fix random seed\n","def same_seeds(seed):\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(seed)\n","        torch.cuda.manual_seed_all(seed)\n","    np.random.seed(seed)\n","    torch.backends.cudnn.benchmark = False\n","    torch.backends.cudnn.deterministic = True"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KbBcBXkSp6RA"},"source":["Feel free to change the training parameters here."]},{"cell_type":"code","metadata":{"id":"QTp3ZXg1yO9Y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616999380117,"user_tz":-480,"elapsed":3090,"user":{"displayName":"Sara Kuo","photoUrl":"","userId":"17378102041433468406"}},"outputId":"80d1c34e-e78e-42d5-ab70-d6b762c3d3f6"},"source":["# get device\n","device = get_device()\n","print(f'DEVICE: {device}')\n","\n","# training parameters\n","num_epoch =  50              # number of training epoch\n","learning_rate = 0.0001       # learning rate"],"execution_count":null,"outputs":[{"output_type":"stream","text":["DEVICE: cuda\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CdMWsBs7zzNs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616928151754,"user_tz":-480,"elapsed":9252051,"user":{"displayName":"Sara Kuo","photoUrl":"","userId":"17378102041433468406"}},"outputId":"95cbae0a-682b-4fed-d072-3649dc91b38b"},"source":["# start training\n","seeds=[7987, 886, 4758, 56879, 49, 76577]\n","# fix random seed for reproducibility\n","for j in range(3) :\n","\n","  same_seeds(seeds[j])\n","\n","  # the path where checkpoint saved\n","  model_path = f'./model{j}.ckpt'\n","  model = Classifier2().to(device)\n","\n","  # create model, define a loss function, and optimizer\n","  # if j%3 == 1 :\n","  #   model = Classifier1().to(device)\n","  # elif j%3 == 2 :\n","  #\n","  # else :\n","  #   model = Classifier3().to(device)\n","\n","  criterion = nn.CrossEntropyLoss()\n","  optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n","\n","  best_acc = 0.0\n","  loss_record = {'train': [], 'dev': []}\n","  print(\"{}th model\".format(j))\n","  for epoch in range(num_epoch):\n","    train_acc = 0.0\n","    train_loss = 0.0\n","    val_acc = 0.0\n","    val_loss = 0.0\n","\n","      # training\n","\n","    model.train() # set the model to training mode\n","    for i, data in enumerate(train_loader):\n","        inputs, labels = data\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        batch_loss = criterion(outputs, labels)\n","        _, train_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n","        batch_loss.backward()\n","        optimizer.step()\n","\n","        train_acc += (train_pred.cpu() == labels.cpu()).sum().item()\n","        train_loss += batch_loss.item()\n","\n","\n","    # validation\n","    if len(val_set) > 0:\n","        model.eval() # set the model to evaluation mode\n","        with torch.no_grad():\n","            for i, data in enumerate(val_loader):\n","                inputs, labels = data\n","                inputs, labels = inputs.to(device), labels.to(device)\n","                outputs = model(inputs)\n","                batch_loss = criterion(outputs, labels)\n","                _, val_pred = torch.max(outputs, 1)\n","\n","                val_acc += (val_pred.cpu() == labels.cpu()).sum().item() # get the index of the class with the highest probability\n","                val_loss += batch_loss.item()\n","\n","\n","            print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f} | Val Acc: {:3.6f} loss: {:3.6f}'.format(\n","                epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader), val_acc/len(val_set), val_loss/len(val_loader)\n","            ))\n","            loss_record['train'].append(train_acc/len(train_set))\n","            loss_record['dev'].append(val_acc/len(val_set))\n","\n","            # if the model improves, save a checkpoint at this epoch\n","            if val_acc > best_acc:\n","                best_acc = val_acc\n","                torch.save(model.state_dict(), model_path)\n","                print('saving model with acc {:.3f}'.format(best_acc/len(val_set)))\n","    else:\n","        print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f}'.format(\n","            epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader)\n","        ))\n","\n","  # if not validating, save the last epoch\n","  if len(val_set) == 0:\n","      torch.save(model.state_dict(), model_path)\n","      print('saving model at last epoch')\n","\n","  print(\"{}th model finished\".format(j))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0th model\n","[001/050] Train Acc: 0.540486 Loss: 1.537680 | Val Acc: 0.665471 loss: 1.114413\n","saving model with acc 0.665\n","[002/050] Train Acc: 0.625342 Loss: 1.234631 | Val Acc: 0.690383 loss: 1.009117\n","saving model with acc 0.690\n","[003/050] Train Acc: 0.652341 Loss: 1.134631 | Val Acc: 0.708116 loss: 0.946599\n","saving model with acc 0.708\n","[004/050] Train Acc: 0.668678 Loss: 1.073739 | Val Acc: 0.716084 loss: 0.920045\n","saving model with acc 0.716\n","[005/050] Train Acc: 0.680220 Loss: 1.030023 | Val Acc: 0.719328 loss: 0.902092\n","saving model with acc 0.719\n","[006/050] Train Acc: 0.689006 Loss: 0.998409 | Val Acc: 0.723800 loss: 0.889491\n","saving model with acc 0.724\n","[007/050] Train Acc: 0.696201 Loss: 0.970285 | Val Acc: 0.730458 loss: 0.868814\n","saving model with acc 0.730\n","[008/050] Train Acc: 0.702875 Loss: 0.947257 | Val Acc: 0.732141 loss: 0.859313\n","saving model with acc 0.732\n","[009/050] Train Acc: 0.708558 Loss: 0.927132 | Val Acc: 0.733849 loss: 0.853472\n","saving model with acc 0.734\n","[010/050] Train Acc: 0.713081 Loss: 0.911191 | Val Acc: 0.736199 loss: 0.843889\n","saving model with acc 0.736\n","[011/050] Train Acc: 0.717325 Loss: 0.894461 | Val Acc: 0.737922 loss: 0.839824\n","saving model with acc 0.738\n","[012/050] Train Acc: 0.721123 Loss: 0.880781 | Val Acc: 0.740020 loss: 0.829181\n","saving model with acc 0.740\n","[013/050] Train Acc: 0.724291 Loss: 0.868705 | Val Acc: 0.743109 loss: 0.826901\n","saving model with acc 0.743\n","[014/050] Train Acc: 0.728554 Loss: 0.855850 | Val Acc: 0.743280 loss: 0.824707\n","saving model with acc 0.743\n","[015/050] Train Acc: 0.731735 Loss: 0.846199 | Val Acc: 0.741931 loss: 0.825080\n","[016/050] Train Acc: 0.734597 Loss: 0.834699 | Val Acc: 0.744207 loss: 0.814712\n","saving model with acc 0.744\n","[017/050] Train Acc: 0.737166 Loss: 0.825430 | Val Acc: 0.745077 loss: 0.819914\n","saving model with acc 0.745\n","[018/050] Train Acc: 0.739057 Loss: 0.817092 | Val Acc: 0.746898 loss: 0.817801\n","saving model with acc 0.747\n","[019/050] Train Acc: 0.742178 Loss: 0.805817 | Val Acc: 0.744679 loss: 0.815165\n","[020/050] Train Acc: 0.744616 Loss: 0.798887 | Val Acc: 0.746435 loss: 0.816309\n","[021/050] Train Acc: 0.747114 Loss: 0.790078 | Val Acc: 0.745679 loss: 0.814732\n","[022/050] Train Acc: 0.748922 Loss: 0.783385 | Val Acc: 0.747321 loss: 0.810287\n","saving model with acc 0.747\n","[023/050] Train Acc: 0.751348 Loss: 0.776980 | Val Acc: 0.748671 loss: 0.812377\n","saving model with acc 0.749\n","[024/050] Train Acc: 0.753102 Loss: 0.770174 | Val Acc: 0.748882 loss: 0.807475\n","saving model with acc 0.749\n","[025/050] Train Acc: 0.755010 Loss: 0.763079 | Val Acc: 0.748126 loss: 0.811504\n","[026/050] Train Acc: 0.756721 Loss: 0.757069 | Val Acc: 0.749142 loss: 0.811015\n","saving model with acc 0.749\n","[027/050] Train Acc: 0.758240 Loss: 0.752085 | Val Acc: 0.748191 loss: 0.812491\n","[028/050] Train Acc: 0.759808 Loss: 0.745364 | Val Acc: 0.749906 loss: 0.814572\n","saving model with acc 0.750\n","[029/050] Train Acc: 0.761586 Loss: 0.740987 | Val Acc: 0.747028 loss: 0.823950\n","[030/050] Train Acc: 0.763489 Loss: 0.735093 | Val Acc: 0.751581 loss: 0.809709\n","saving model with acc 0.752\n","[031/050] Train Acc: 0.764222 Loss: 0.731068 | Val Acc: 0.748126 loss: 0.814236\n","[032/050] Train Acc: 0.766118 Loss: 0.723331 | Val Acc: 0.749524 loss: 0.813243\n","[033/050] Train Acc: 0.767530 Loss: 0.720596 | Val Acc: 0.750142 loss: 0.818050\n","[034/050] Train Acc: 0.769054 Loss: 0.715731 | Val Acc: 0.749955 loss: 0.811726\n","[035/050] Train Acc: 0.770750 Loss: 0.709694 | Val Acc: 0.749972 loss: 0.813610\n","[036/050] Train Acc: 0.771139 Loss: 0.706625 | Val Acc: 0.749972 loss: 0.812535\n","[037/050] Train Acc: 0.772248 Loss: 0.702366 | Val Acc: 0.750004 loss: 0.817305\n","[038/050] Train Acc: 0.773654 Loss: 0.698914 | Val Acc: 0.751029 loss: 0.814154\n","[039/050] Train Acc: 0.775235 Loss: 0.693845 | Val Acc: 0.750207 loss: 0.817164\n","[040/050] Train Acc: 0.776504 Loss: 0.690758 | Val Acc: 0.752061 loss: 0.812253\n","saving model with acc 0.752\n","[041/050] Train Acc: 0.777356 Loss: 0.687246 | Val Acc: 0.751337 loss: 0.818999\n","[042/050] Train Acc: 0.778600 Loss: 0.683517 | Val Acc: 0.751663 loss: 0.818751\n","[043/050] Train Acc: 0.778788 Loss: 0.680092 | Val Acc: 0.751923 loss: 0.820322\n","[044/050] Train Acc: 0.780702 Loss: 0.675005 | Val Acc: 0.750614 loss: 0.825816\n","[045/050] Train Acc: 0.781449 Loss: 0.671798 | Val Acc: 0.751077 loss: 0.824897\n","[046/050] Train Acc: 0.782087 Loss: 0.670310 | Val Acc: 0.750866 loss: 0.830748\n","[047/050] Train Acc: 0.783124 Loss: 0.665556 | Val Acc: 0.751240 loss: 0.826511\n","[048/050] Train Acc: 0.784031 Loss: 0.664300 | Val Acc: 0.750346 loss: 0.834101\n","[049/050] Train Acc: 0.785011 Loss: 0.661187 | Val Acc: 0.751167 loss: 0.827444\n","[050/050] Train Acc: 0.785786 Loss: 0.657558 | Val Acc: 0.750923 loss: 0.837543\n","0th model finished\n","1th model\n","[001/050] Train Acc: 0.541206 Loss: 1.532329 | Val Acc: 0.661211 loss: 1.113919\n","saving model with acc 0.661\n","[002/050] Train Acc: 0.625282 Loss: 1.234524 | Val Acc: 0.695107 loss: 1.000632\n","saving model with acc 0.695\n","[003/050] Train Acc: 0.652086 Loss: 1.135238 | Val Acc: 0.708116 loss: 0.940694\n","saving model with acc 0.708\n","[004/050] Train Acc: 0.668705 Loss: 1.074766 | Val Acc: 0.714913 loss: 0.919366\n","saving model with acc 0.715\n","[005/050] Train Acc: 0.680324 Loss: 1.030093 | Val Acc: 0.720848 loss: 0.897521\n","saving model with acc 0.721\n","[006/050] Train Acc: 0.689717 Loss: 0.996738 | Val Acc: 0.726304 loss: 0.883607\n","saving model with acc 0.726\n","[007/050] Train Acc: 0.697146 Loss: 0.970245 | Val Acc: 0.729271 loss: 0.869657\n","saving model with acc 0.729\n","[008/050] Train Acc: 0.703627 Loss: 0.946590 | Val Acc: 0.732458 loss: 0.861622\n","saving model with acc 0.732\n","[009/050] Train Acc: 0.708707 Loss: 0.926283 | Val Acc: 0.732792 loss: 0.850143\n","saving model with acc 0.733\n","[010/050] Train Acc: 0.713446 Loss: 0.909609 | Val Acc: 0.738239 loss: 0.837525\n","saving model with acc 0.738\n","[011/050] Train Acc: 0.717877 Loss: 0.894264 | Val Acc: 0.738394 loss: 0.837715\n","saving model with acc 0.738\n","[012/050] Train Acc: 0.722183 Loss: 0.879031 | Val Acc: 0.739963 loss: 0.829272\n","saving model with acc 0.740\n","[013/050] Train Acc: 0.725457 Loss: 0.867172 | Val Acc: 0.740215 loss: 0.829724\n","saving model with acc 0.740\n","[014/050] Train Acc: 0.728692 Loss: 0.854500 | Val Acc: 0.740776 loss: 0.824040\n","saving model with acc 0.741\n","[015/050] Train Acc: 0.731913 Loss: 0.843611 | Val Acc: 0.741150 loss: 0.824622\n","saving model with acc 0.741\n","[016/050] Train Acc: 0.734623 Loss: 0.834162 | Val Acc: 0.744240 loss: 0.818303\n","saving model with acc 0.744\n","[017/050] Train Acc: 0.737132 Loss: 0.824934 | Val Acc: 0.741589 loss: 0.821365\n","[018/050] Train Acc: 0.740225 Loss: 0.814197 | Val Acc: 0.744939 loss: 0.817072\n","saving model with acc 0.745\n","[019/050] Train Acc: 0.742961 Loss: 0.806292 | Val Acc: 0.743963 loss: 0.823220\n","[020/050] Train Acc: 0.745327 Loss: 0.797930 | Val Acc: 0.744549 loss: 0.814388\n","[021/050] Train Acc: 0.747297 Loss: 0.790885 | Val Acc: 0.746914 loss: 0.810292\n","saving model with acc 0.747\n","[022/050] Train Acc: 0.749192 Loss: 0.783369 | Val Acc: 0.746118 loss: 0.813812\n","[023/050] Train Acc: 0.751454 Loss: 0.776240 | Val Acc: 0.747004 loss: 0.811838\n","saving model with acc 0.747\n","[024/050] Train Acc: 0.752998 Loss: 0.771170 | Val Acc: 0.746768 loss: 0.809841\n","[025/050] Train Acc: 0.754493 Loss: 0.764875 | Val Acc: 0.747280 loss: 0.810747\n","saving model with acc 0.747\n","[026/050] Train Acc: 0.756695 Loss: 0.757104 | Val Acc: 0.747955 loss: 0.806446\n","saving model with acc 0.748\n","[027/050] Train Acc: 0.757991 Loss: 0.751443 | Val Acc: 0.745914 loss: 0.817358\n","[028/050] Train Acc: 0.759846 Loss: 0.745042 | Val Acc: 0.747378 loss: 0.810394\n","[029/050] Train Acc: 0.762093 Loss: 0.740732 | Val Acc: 0.746988 loss: 0.816056\n","[030/050] Train Acc: 0.763335 Loss: 0.734785 | Val Acc: 0.749492 loss: 0.812858\n","saving model with acc 0.749\n","[031/050] Train Acc: 0.764355 Loss: 0.729814 | Val Acc: 0.747752 loss: 0.807741\n","[032/050] Train Acc: 0.765832 Loss: 0.725519 | Val Acc: 0.748833 loss: 0.813030\n","[033/050] Train Acc: 0.767914 Loss: 0.719739 | Val Acc: 0.749386 loss: 0.808595\n","[034/050] Train Acc: 0.769294 Loss: 0.714734 | Val Acc: 0.748305 loss: 0.815299\n","[035/050] Train Acc: 0.769629 Loss: 0.711721 | Val Acc: 0.747858 loss: 0.808886\n","[036/050] Train Acc: 0.771760 Loss: 0.706176 | Val Acc: 0.747719 loss: 0.817057\n","[037/050] Train Acc: 0.772337 Loss: 0.702980 | Val Acc: 0.747817 loss: 0.816473\n","[038/050] Train Acc: 0.774375 Loss: 0.697579 | Val Acc: 0.750297 loss: 0.815030\n","saving model with acc 0.750\n","[039/050] Train Acc: 0.774900 Loss: 0.694884 | Val Acc: 0.749093 loss: 0.817070\n","[040/050] Train Acc: 0.776211 Loss: 0.689485 | Val Acc: 0.748923 loss: 0.819451\n","[041/050] Train Acc: 0.777073 Loss: 0.687994 | Val Acc: 0.749882 loss: 0.817079\n","[042/050] Train Acc: 0.778537 Loss: 0.683439 | Val Acc: 0.749362 loss: 0.821710\n","[043/050] Train Acc: 0.779239 Loss: 0.679711 | Val Acc: 0.750606 loss: 0.820079\n","saving model with acc 0.751\n","[044/050] Train Acc: 0.780270 Loss: 0.677266 | Val Acc: 0.749435 loss: 0.819590\n","[045/050] Train Acc: 0.781227 Loss: 0.672912 | Val Acc: 0.751142 loss: 0.816260\n","saving model with acc 0.751\n","[046/050] Train Acc: 0.782174 Loss: 0.669523 | Val Acc: 0.749841 loss: 0.820079\n","[047/050] Train Acc: 0.783296 Loss: 0.666854 | Val Acc: 0.750094 loss: 0.821661\n","[048/050] Train Acc: 0.784369 Loss: 0.663834 | Val Acc: 0.750679 loss: 0.828982\n","[049/050] Train Acc: 0.784559 Loss: 0.660559 | Val Acc: 0.750573 loss: 0.823163\n","[050/050] Train Acc: 0.785609 Loss: 0.657592 | Val Acc: 0.750882 loss: 0.824215\n","1th model finished\n","2th model\n","[001/050] Train Acc: 0.539571 Loss: 1.538527 | Val Acc: 0.663317 loss: 1.112469\n","saving model with acc 0.663\n","[002/050] Train Acc: 0.624073 Loss: 1.236525 | Val Acc: 0.692058 loss: 0.997595\n","saving model with acc 0.692\n","[003/050] Train Acc: 0.652148 Loss: 1.137111 | Val Acc: 0.705116 loss: 0.949261\n","saving model with acc 0.705\n","[004/050] Train Acc: 0.668926 Loss: 1.075058 | Val Acc: 0.714059 loss: 0.926025\n","saving model with acc 0.714\n","[005/050] Train Acc: 0.680219 Loss: 1.031472 | Val Acc: 0.720198 loss: 0.893357\n","saving model with acc 0.720\n","[006/050] Train Acc: 0.689556 Loss: 0.998097 | Val Acc: 0.723686 loss: 0.887030\n","saving model with acc 0.724\n","[007/050] Train Acc: 0.696937 Loss: 0.970430 | Val Acc: 0.729027 loss: 0.869744\n","saving model with acc 0.729\n","[008/050] Train Acc: 0.703152 Loss: 0.947730 | Val Acc: 0.729206 loss: 0.859035\n","saving model with acc 0.729\n","[009/050] Train Acc: 0.708459 Loss: 0.928609 | Val Acc: 0.734247 loss: 0.853542\n","saving model with acc 0.734\n","[010/050] Train Acc: 0.713387 Loss: 0.910926 | Val Acc: 0.735963 loss: 0.845940\n","saving model with acc 0.736\n","[011/050] Train Acc: 0.717196 Loss: 0.895261 | Val Acc: 0.740060 loss: 0.832489\n","saving model with acc 0.740\n","[012/050] Train Acc: 0.721682 Loss: 0.881307 | Val Acc: 0.737938 loss: 0.844024\n","[013/050] Train Acc: 0.725680 Loss: 0.868191 | Val Acc: 0.741313 loss: 0.832902\n","saving model with acc 0.741\n","[014/050] Train Acc: 0.728308 Loss: 0.856140 | Val Acc: 0.742492 loss: 0.830926\n","saving model with acc 0.742\n","[015/050] Train Acc: 0.732016 Loss: 0.844488 | Val Acc: 0.743475 loss: 0.820922\n","saving model with acc 0.743\n","[016/050] Train Acc: 0.734364 Loss: 0.834619 | Val Acc: 0.742069 loss: 0.828589\n","[017/050] Train Acc: 0.738451 Loss: 0.824068 | Val Acc: 0.744109 loss: 0.825365\n","saving model with acc 0.744\n","[018/050] Train Acc: 0.739854 Loss: 0.814682 | Val Acc: 0.743248 loss: 0.823909\n","[019/050] Train Acc: 0.742481 Loss: 0.807418 | Val Acc: 0.745744 loss: 0.821670\n","saving model with acc 0.746\n","[020/050] Train Acc: 0.745036 Loss: 0.798345 | Val Acc: 0.745337 loss: 0.821996\n","[021/050] Train Acc: 0.746450 Loss: 0.792254 | Val Acc: 0.745809 loss: 0.821375\n","saving model with acc 0.746\n","[022/050] Train Acc: 0.748549 Loss: 0.784669 | Val Acc: 0.745223 loss: 0.825043\n","[023/050] Train Acc: 0.751490 Loss: 0.775763 | Val Acc: 0.747679 loss: 0.815897\n","saving model with acc 0.748\n","[024/050] Train Acc: 0.753030 Loss: 0.769986 | Val Acc: 0.747345 loss: 0.820136\n","[025/050] Train Acc: 0.755233 Loss: 0.762076 | Val Acc: 0.745110 loss: 0.821413\n","[026/050] Train Acc: 0.756869 Loss: 0.756699 | Val Acc: 0.747199 loss: 0.814600\n","[027/050] Train Acc: 0.758166 Loss: 0.752463 | Val Acc: 0.747955 loss: 0.821701\n","saving model with acc 0.748\n","[028/050] Train Acc: 0.759887 Loss: 0.746541 | Val Acc: 0.746858 loss: 0.821431\n","[029/050] Train Acc: 0.761660 Loss: 0.739701 | Val Acc: 0.747256 loss: 0.816901\n","[030/050] Train Acc: 0.763503 Loss: 0.733958 | Val Acc: 0.748573 loss: 0.825906\n","saving model with acc 0.749\n","[031/050] Train Acc: 0.764700 Loss: 0.729153 | Val Acc: 0.748646 loss: 0.819362\n","saving model with acc 0.749\n","[032/050] Train Acc: 0.765935 Loss: 0.724522 | Val Acc: 0.749728 loss: 0.824190\n","saving model with acc 0.750\n","[033/050] Train Acc: 0.767122 Loss: 0.720061 | Val Acc: 0.749224 loss: 0.824049\n","[034/050] Train Acc: 0.768493 Loss: 0.715882 | Val Acc: 0.748784 loss: 0.829285\n","[035/050] Train Acc: 0.770505 Loss: 0.709805 | Val Acc: 0.750094 loss: 0.824610\n","saving model with acc 0.750\n","[036/050] Train Acc: 0.771530 Loss: 0.705777 | Val Acc: 0.750077 loss: 0.826510\n","[037/050] Train Acc: 0.772854 Loss: 0.700969 | Val Acc: 0.749248 loss: 0.830324\n","[038/050] Train Acc: 0.773617 Loss: 0.698296 | Val Acc: 0.750337 loss: 0.823851\n","saving model with acc 0.750\n","[039/050] Train Acc: 0.774714 Loss: 0.693693 | Val Acc: 0.750468 loss: 0.827780\n","saving model with acc 0.750\n","[040/050] Train Acc: 0.776652 Loss: 0.688870 | Val Acc: 0.750224 loss: 0.828492\n","[041/050] Train Acc: 0.776968 Loss: 0.685657 | Val Acc: 0.749671 loss: 0.828792\n","[042/050] Train Acc: 0.778087 Loss: 0.682765 | Val Acc: 0.749785 loss: 0.826514\n","[043/050] Train Acc: 0.779235 Loss: 0.678326 | Val Acc: 0.749378 loss: 0.838563\n","[044/050] Train Acc: 0.780132 Loss: 0.676025 | Val Acc: 0.749898 loss: 0.839834\n","[045/050] Train Acc: 0.781634 Loss: 0.671131 | Val Acc: 0.749785 loss: 0.831047\n","[046/050] Train Acc: 0.782357 Loss: 0.668886 | Val Acc: 0.751500 loss: 0.831463\n","saving model with acc 0.752\n","[047/050] Train Acc: 0.783714 Loss: 0.666022 | Val Acc: 0.750955 loss: 0.839571\n","[048/050] Train Acc: 0.783708 Loss: 0.663393 | Val Acc: 0.750459 loss: 0.848181\n","[049/050] Train Acc: 0.785575 Loss: 0.659551 | Val Acc: 0.750297 loss: 0.842834\n","[050/050] Train Acc: 0.785335 Loss: 0.657318 | Val Acc: 0.750402 loss: 0.839915\n","2th model finished\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TgVTfAuzGJ01"},"source":["import matplotlib.pyplot as plt\n","from matplotlib.pyplot import figure\n","\n","def plot_learning_curve(loss_record, title=''):\n","    ''' Plot learning curve of your DNN (train & dev loss) '''\n","    total_steps = len(loss_record['train'])\n","    x_1 = range(total_steps)\n","    x_2 = x_1[::len(loss_record['train']) // len(loss_record['dev'])]\n","    figure(figsize=(6, 4))\n","\n","    plt.plot(x_1, loss_record['train'], c='tab:red', label='train')\n","    plt.plot(x_2, loss_record['dev'], c='tab:cyan', label='dev')\n","    plt.ylim(0.5, 1.)\n","    plt.xlabel('Training steps')\n","    plt.ylabel('MSE loss')\n","    plt.title('Learning curve of {}'.format(title))\n","    plt.legend()\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EcLVLE2QGajy"},"source":["plot_learning_curve(loss_record, title='deep model')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1Hi7jTn3PX-m"},"source":["## Testing"]},{"cell_type":"markdown","metadata":{"id":"NfUECMFCn5VG"},"source":["Create a testing dataset, and load model from the saved checkpoint."]},{"cell_type":"code","metadata":{"id":"1PKjtAScPWtr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617005263184,"user_tz":-480,"elapsed":714825,"user":{"displayName":"Sara Kuo","photoUrl":"","userId":"17378102041433468406"}},"outputId":"0b26ec46-3746-4d72-9bcb-9f88b5dab0b9"},"source":["# create testing dataset\n","test_set = TIMITDataset(test, None)\n","test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n","\n","# create model and load weights from checkpoint\n","model = Classifier1().to(device)\n","possibility = np.zeros((451552,39))\n","\n","for i in range(9):\n","  print(i)\n","  j=i\n","  if j < 3 :\n","    model = Classifier1().to(device)\n","\n","  elif j < 6 :\n","    model = Classifier2().to(device)\n","\n","  else :\n","    model = Classifier3().to(device)\n","\n","  model.load_state_dict(torch.load(f'./model{i}.ckpt'))\n","  model.eval() # set the model to evaluation mode\n","\n","  with torch.no_grad():\n","      for i, data in enumerate(test_loader):\n","          inputs = data\n","          inputs = inputs.to(device)\n","          outputs = model(inputs).cpu().numpy()\n","          p_bar = np.zeros(outputs.shape)\n","          for ind, j in enumerate(np.argmax(outputs, axis=1)):\n","            p_bar[ind][j] = 1\n","\n","          # p_bar[np.arange(128),np.argmax(outputs, axis=1).reshape(-1,1)] = 1\n","\n","\n","\n","          if i == 0 :\n","            # print(\"out\",outputs, outputs.shape)\n","            # print(np.argmax(outputs, axis=1), np.argmax(outputs, axis=1).shape)\n","            # print(\"p_bar\",p_bar, p_bar.shape)\n","            p = p_bar\n","            # print(np.shape(outputs))\n","          else :\n","            p = np.append(p, p_bar, axis =0)\n","            # print(\"outputs\",np.shape(outputs))\n","            # print(\"p\",np.shape(p))\n","\n","\n","  # print(np.shape(p))\n","  possibility += p\n","  # print(possibility)\n","\n","predict = np.argmax(possibility, axis =1)\n","\n","# _, test_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n","\n","#           for y in test_pred.cpu().numpy():\n","#               predict.append(y)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0\n","[[0. 0. 0. ... 1. 0. 0.]\n"," [0. 0. 0. ... 1. 0. 0.]\n"," [0. 0. 0. ... 1. 0. 0.]\n"," ...\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]]\n","1\n","[[0. 0. 0. ... 2. 0. 0.]\n"," [0. 0. 0. ... 2. 0. 0.]\n"," [0. 0. 0. ... 2. 0. 0.]\n"," ...\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]]\n","2\n","[[0. 0. 0. ... 3. 0. 0.]\n"," [0. 0. 0. ... 3. 0. 0.]\n"," [0. 0. 0. ... 3. 0. 0.]\n"," ...\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]]\n","3\n","[[0. 0. 0. ... 4. 0. 0.]\n"," [0. 0. 0. ... 4. 0. 0.]\n"," [0. 0. 0. ... 4. 0. 0.]\n"," ...\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]]\n","4\n","[[0. 0. 0. ... 5. 0. 0.]\n"," [0. 0. 0. ... 5. 0. 0.]\n"," [0. 0. 0. ... 5. 0. 0.]\n"," ...\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]]\n","5\n","[[0. 0. 0. ... 6. 0. 0.]\n"," [0. 0. 0. ... 6. 0. 0.]\n"," [0. 0. 0. ... 6. 0. 0.]\n"," ...\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]]\n","6\n","[[0. 0. 0. ... 7. 0. 0.]\n"," [0. 0. 0. ... 7. 0. 0.]\n"," [0. 0. 0. ... 7. 0. 0.]\n"," ...\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]]\n","7\n","[[0. 0. 0. ... 8. 0. 0.]\n"," [0. 0. 0. ... 8. 0. 0.]\n"," [0. 0. 0. ... 8. 0. 0.]\n"," ...\n"," [0. 0. 0. ... 0. 0. 1.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]]\n","8\n","[[0. 0. 0. ... 9. 0. 0.]\n"," [0. 0. 0. ... 9. 0. 0.]\n"," [0. 0. 0. ... 9. 0. 0.]\n"," ...\n"," [0. 0. 0. ... 0. 0. 1.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"940TtCCdoYd0"},"source":["Make prediction."]},{"cell_type":"markdown","metadata":{"id":"AWDf_C-omElb"},"source":["Write prediction to a CSV file.\n","\n","After finish running this block, download the file `prediction.csv` from the files section on the left-hand side and submit it to Kaggle."]},{"cell_type":"code","metadata":{"id":"GuljYSPHcZir"},"source":["with open('prediction.csv', 'w') as f:\n","    f.write('Id,Class\\n')\n","    for i, y in enumerate(predict):\n","        f.write('{},{}\\n'.format(i, y))"],"execution_count":null,"outputs":[]}]}